{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup/Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pylab import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import Caffe using 'caffe_root'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "caffe_root = '/Users/Kallie/caffe'\n",
    "\n",
    "sys.path.insert(0, '/Users/Kallie/anaconda3/envs/final/lib')\n",
    "\n",
    "#Add path for python layers\n",
    "sys.path.insert(0, '/Users/Kallie/caffe/python/caffe/layers')\n",
    "\n",
    "sys.path.insert(0, caffe_root + 'python')\n",
    "import caffe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build LeNet Architecture for MNSIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LeNet no Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caffe import layers as L, params as P\n",
    "\n",
    "def lenet_sparse(lmdb, batch_size):\n",
    "    # our version of LeNet: a series of linear and simple nonlinear transformations\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,\n",
    "                             transform_param=dict(scale=1./255), ntop=2)\n",
    "    \n",
    "    n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=20, weight_filler=dict(type='xavier'))\n",
    "    n.pool1 = L.Pooling(n.conv1, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=50, weight_filler=dict(type='xavier'))\n",
    "    n.pool2 = L.Pooling(n.conv2, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.fc1 =   L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type='xavier'))\n",
    "    n.relu1 = L.ReLU(n.fc1, in_place=True)\n",
    "    n.score = L.InnerProduct(n.relu1, num_output=10, weight_filler=dict(type='xavier'))\n",
    "    n.loss =  L.SoftmaxWithLoss(n.score, n.label)\n",
    "    \n",
    "    return n.to_proto()\n",
    "    \n",
    "with open('mnist/lenet_sparse_train.prototxt', 'w') as f:\n",
    "    f.write(str(lenet_sparse('mnist/mnist_train_lmdb', 64)))\n",
    "    \n",
    "with open('mnist/lenet_sparse_test.prototxt', 'w') as f:\n",
    "    f.write(str(lenet_sparse('mnist/mnist_test_lmdb', 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LeNet with All Vanilla Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caffe import layers as L, params as P\n",
    "\n",
    "def lenet_all_vanilla_sparse(lmdb, batch_size):\n",
    "    # our version of LeNet: a series of linear and simple nonlinear transformations\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,\n",
    "                             transform_param=dict(scale=1./255), ntop=2)\n",
    "    \n",
    "    n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=40, weight_filler=dict(type='xavier'))\n",
    "    n.drop1 = L.Dropout(n.conv1, dropout_ratio=0.5)\n",
    "    n.pool1 = L.Pooling(n.drop1, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=100, weight_filler=dict(type='xavier'))\n",
    "    n.drop2 = L.Dropout(n.conv2, dropout_ratio=0.5)\n",
    "    n.pool2 = L.Pooling(n.drop2, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.fc1 =   L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type='xavier'))\n",
    "    n.relu1 = L.ReLU(n.fc1, in_place=True)\n",
    "    n.score = L.InnerProduct(n.relu1, num_output=10, weight_filler=dict(type='xavier'))\n",
    "    n.loss =  L.SoftmaxWithLoss(n.score, n.label)\n",
    "    \n",
    "    return n.to_proto()\n",
    "    \n",
    "with open('mnist/lenet_all_vanilla_sparse_auto_train.prototxt', 'w') as f:\n",
    "    f.write(str(lenet_all_vanilla_sparse('mnist/mnist_train_lmdb', 64)))\n",
    "    \n",
    "with open('mnist/lenet_all_vanilla_sparse_auto_test.prototxt', 'w') as f:\n",
    "    f.write(str(lenet_all_vanilla_sparse('mnist/mnist_test_lmdb', 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LenNet With All Modified Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caffe import layers as L, params as P\n",
    "\n",
    "def lenet_all_mod_sparse(lmdb, batch_size):\n",
    "    # our version of LeNet: a series of linear and simple nonlinear transformations\n",
    "    n = caffe.NetSpec()\n",
    "    \n",
    "    n.data, n.label = L.Data(batch_size=batch_size, backend=P.Data.LMDB, source=lmdb,\n",
    "                             transform_param=dict(scale=1./255), ntop=2)\n",
    "    \n",
    "    n.conv1 = L.Convolution(n.data, kernel_size=5, num_output=20, weight_filler=dict(type='xavier'))\n",
    "    n.drop1 = L.Python(n.conv1, name='drop1', ntop=1, python_param={'module': 'Dropout_M',\n",
    "                           'layer': 'Dropout_M_Layer'})\n",
    "    n.pool1 = L.Pooling(n.drop1, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.conv2 = L.Convolution(n.pool1, kernel_size=5, num_output=50, weight_filler=dict(type='xavier'))\n",
    "    n.drop2 = L.Python(n.conv2, name='drop1', ntop=1, python_param={'module': 'Dropout_M',\n",
    "                           'layer': 'Dropout_M_Layer'})\n",
    "    n.pool2 = L.Pooling(n.drop2, kernel_size=2, stride=2, pool=P.Pooling.MAX)\n",
    "    n.fc1 =   L.InnerProduct(n.pool2, num_output=500, weight_filler=dict(type='xavier'))\n",
    "    n.relu1 = L.ReLU(n.fc1, in_place=True)\n",
    "    n.score = L.InnerProduct(n.relu1, num_output=10, weight_filler=dict(type='xavier'))\n",
    "    n.loss =  L.SoftmaxWithLoss(n.score, n.label)\n",
    "    \n",
    "    return n.to_proto()\n",
    "    \n",
    "with open('mnist/lenet_all_mod_sparse_auto_train.prototxt', 'w') as f:\n",
    "    f.write(str(lenet_all_mod_sparse('mnist/mnist_train_lmdb', 64)))\n",
    "    \n",
    "with open('mnist/lenet_all_mod_sparse_auto_test.prototxt', 'w') as f:\n",
    "    f.write(str(lenet_all_mod_sparse('mnist/mnist_test_lmdb', 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Solver\n",
    "* Change Solver based on LeNet You Want to Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "caffe.set_mode_cpu()\n",
    "\n",
    "### load the solver and create train and test nets\n",
    "solver = None  # ignore this workaround for lmdb data (can't instantiate two solvers on the same data)\n",
    "\n",
    "# 'mnist/lenet_sparse_solver.prototxt'\n",
    "# 'mnist/lenet_all_mod_sparse_auto_solver.prototxt'\n",
    "solver = caffe.SGDSolver('mnist/lenet_all_mod_sparse_auto_solver.prototxt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity Checks\n",
    "* output layer sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data', (64, 1, 28, 28)),\n",
       " ('label', (64,)),\n",
       " ('conv1', (64, 20, 24, 24)),\n",
       " ('drop1', (64, 20, 24, 24)),\n",
       " ('pool1', (64, 20, 12, 12)),\n",
       " ('conv2', (64, 50, 8, 8)),\n",
       " ('drop2', (64, 50, 8, 8)),\n",
       " ('pool2', (64, 50, 4, 4)),\n",
       " ('fc1', (64, 500)),\n",
       " ('score', (64, 10)),\n",
       " ('loss', ())]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# each output is (batch size, feature dim, spatial dim)\n",
    "[(k, v.data.shape) for k, v in solver.net.blobs.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('conv1', (20, 1, 5, 5)),\n",
       " ('conv2', (50, 20, 5, 5)),\n",
       " ('fc1', (500, 800)),\n",
       " ('score', (10, 500))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just print the weight sizes (we'll omit the biases)\n",
    "[(k, v[0].data.shape) for k, v in solver.net.params.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Loss looks Correct and Check Data is Loaded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kallie/caffe/python/caffe/layers/Dropout_M.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  eps = sigmoid(np.log(data**2))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': array(3.1977205, dtype=float32)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solver.net.forward()  # train net\n",
    "solver.test_nets[0].forward()  # test net (there can be more than one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Kallie/caffe/python/caffe/layers/Dropout_M.py:14: RuntimeWarning: divide by zero encountered in log\n",
      "  eps = sigmoid(np.log(data**2))\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "niter =  2000\n",
    "test_interval = 100\n",
    "ep = 1\n",
    "mod_list = []\n",
    "# losses will also be stored in the log\n",
    "train_loss = zeros(niter)\n",
    "test_acc = zeros(int(np.ceil(niter / test_interval)))\n",
    "output = zeros((niter, 8, 10))\n",
    "\n",
    "# the main solver loop\n",
    "for it in range(niter):\n",
    "    solver.step(1)  # SGD by Caffe\n",
    "    \n",
    "    # store the train loss\n",
    "    train_loss[it] = solver.net.blobs['loss'].data\n",
    "    \n",
    "    # store the output on the first test batch\n",
    "    # (start the forward pass at conv1 to avoid loading new data)\n",
    "    solver.test_nets[0].forward(start='conv1')\n",
    "    output[it] = solver.test_nets[0].blobs['score'].data[:8]\n",
    "    \n",
    "    # run a full test every so often\n",
    "    # (Caffe can also do this for us and write to a log, but we show here\n",
    "    #  how to do it directly in Python, where more complicated things are easier.)\n",
    "    if it % test_interval == 0:\n",
    "        #print('Iteration', it, 'testing...')\n",
    "        correct = 0\n",
    "        for test_it in range(100):\n",
    "            solver.test_nets[0].forward()\n",
    "            correct += sum(solver.test_nets[0].blobs['score'].data.argmax(1)\n",
    "                           == solver.test_nets[0].blobs['label'].data)\n",
    "        test_acc[it // test_interval] = correct / 1e4\n",
    "     \n",
    "    \n",
    "    if it % 156 == 0:\n",
    "\n",
    "        layer1  = solver.net.blobs['drop1'].data.copy()\n",
    "        layer2  = solver.net.blobs['drop2'].data.copy()\n",
    "        \n",
    "        print(solver.net.blobs['drop1'].items())\n",
    "        \n",
    "        layer1[abs(layer1)<1e-9] = 0\n",
    "        layer2[abs(layer2)<1e-9] = 0\n",
    "        \n",
    "        layer1_not_zero = np.count_nonzero(layer1)\n",
    "        layer2_not_zero = np.count_nonzero(layer2)\n",
    "        \n",
    "        nom = (np.prod(layer1.shape) + np.prod(layer2.shape))\n",
    "        denom = (layer1_not_zero + layer2_not_zero)\n",
    "        \n",
    "        sparse = nom/denom\n",
    "        \n",
    "        print('Sparsity of Epoch ' + str(ep)+ ' is: ' + str(sparse))\n",
    "        mod_list.append(sparse)\n",
    "        ep += 1\n",
    "mod = np.mean(mod_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax1 = subplots()\n",
    "print(mod)\n",
    "mods = []\n",
    "lenets = [1,1,1,1]\n",
    "vanillas = [2.0783538235023276, 2.0784431406696475, 2.0778395625890305, 2.0788826576561448]\n",
    "ax1.plot([.25, .5, 1, 1.5], mods, 'b+-', label='Modified Dropout')\n",
    "ax1.plot([.25, .5, 1, 1.5], [.25, .5, 1, 1.5], 'c+-', label='No Dropout')\n",
    "ax1.plot([.25, .5, 1, 1.5], [.25, .5, 1, 1.5], 'g+-', label='Vanilla Dropout')\n",
    "ax1.set_xlabel('Scaled Number of neurons')\n",
    "ax1.legend()\n",
    "ax1.set_ylabel('Sparsity')\n",
    "\n",
    "\n",
    "#savefig('mnist/figures/sparsity_mnist.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final",
   "language": "python",
   "name": "final"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
